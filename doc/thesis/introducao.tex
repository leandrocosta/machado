\chapter{Introdução}
\label{chapter:introducao}

Há muito já não é mais novidade o fato de que vivemos, hoje, na chamada \textbf{Era da Informação} (termo cunhado por Daniel Bell, professor emérito da Universidade de Harvard), que se define pelo rápido desenvolvimento da tecnologia de informação, característica marcante da \textbf{Sociedade Pós-Industrial}, onde nota-se que, mais importante que possuir a informação, é saber onde e como encontrá-la.
\par
Ao longo dos últimos anos, o desenvolvimento tecnológico fez com que o constante armazenamento de dados se tornasse uma atividade fácil e de baixo custo. A conseqüência deste fato foi a alimentação de enormes bases de dados, o que proporcionou a  necessidade de se pensar em novas soluções de gerência, manutenção e, posteriormente, acesso.
\par
Os Sistemas de Gerenciamento de Banco de Dados (\textbf{SGBD}) surgiram como uma solução para gerência e manutenção de bases, com o objetivo de retirar da aplicação cliente a responsabilidade de administrar o acesso, a manipulação e a organização dos dados. Entretanto, embora os complexos SGBD's tenham se consolidado como eficientes sistemas na gerência de grandes volumes de dados, e tratado com eficácia a recuperação de informações em amplas coleções, da existência de tais bases surgiu a necessidade de métodos mais avançados de recuperação de informação, como redução de volume de dados, extração da essência da informação armazenada, descoberta de padrões, dentre outros.
\par
Como resposta à necessidade dos novos métodos de recuperação de informação, surgiu o que chamamos hoje de \textbf{Mineração de Dados}.

\section{Mineração de Dados}

Mineração de dados é o resultado de um longo processo de pesquisa e desenvolvimento, considerado uma das mais importantes fronteiras entre bases de dados e sistemas de informação, e um dos mais promissores desenvolvimentos interdisciplinares na tecnologia da informação \citep{Han00}. Podemos defini-la como o conjunto de métodos não triviais de análise de dados e extração de informação potencialmente útil, implícita, previamente desconhecida, e suas novas formas de representação e visualização \citep{Kumar06, Hand01, PiatetskyF91}.
\par
O termo Mineração de Dados é mencionado com freqüência no contexto de \textbf{Descoberta de Conhecimento em Bases de Dados} (\textbf{KDD - \textit{Knowledge Discovery in Databases}}), que é um processo de extração de conhecimentos válidos, novos, potencialmente úteis e compreensíveis para apoiar a tomada de decisão \citep{Fayyad96}. Para tanto, o KDD faz uso de diversos artifícios, incluindo métodos estatísticos, reconhecimento de padrões, visualização, banco de dados, aprendizado de máquina, inteligência artificial, \textit{data warehouse}, dentre outros \citep{Sassi06}.
\par
O processo de descoberta de conhecimento em bases de dados consiste, usualmente, na seguinte seqüência de etapas:

\begin{enumerate}
	\item{Aprendizado:} Entendimento do domínio da aplicação, que inclui conhecimento prévio da aplicação e dos seus objetivos;
	\item{Seleção:} Coleta dos dados, constituído pela definição de uma base de dados e pela seleção de um conjunto de dados e atributos a serem considerados;
	\item{Pré-processamento:} Abrange a execução de operações básicas como remoção de ruídos, a definição de procedimentos e estratégias aplicáveis a dados faltosos, decisões relacionadas a tipos de dados, dentre outras tarefas;
	\item{Transformação:} Consiste na definição de atributos indicados para representar os dados, na definição da dimensionalidade dos dados, e em outros métodos de transformação para reduzir o número de variáveis sob consideração;
	\item{Mineração de Dados:} Inclui a escolha da função de mineração de dados, de acordo com o propósito do modelo (podendo ser classificação, agrupamento, sumarização ou até mesmo combinações de duas ou mais funções quaisquer), e do algoritmo a ser aplicado, que depende de decisões como qual modelo é mais apropriado considerando-se a natureza dos dados. Por exemplo, modelos para dados categóricos são diferentes de modelos para dados contínuos. A execução da mineração de dados se resume em pesquisar por padrões de interesse numa representação (ou conjunto de representações) definida de acordo com a função de mineração de dados escolhida;
	\item{Interpretação e Avaliação:} Esta etapa pode resultar no retorno a qualquer um dos passos anteriores, caso os resultados não sejam satisfatórios e alguma alteração seja necessária para a sua melhoria;
	\item{Utilização do Conhecimento:} Consiste na incorporação do conhecimento adquirido promovendo ações ou simplesmente documentando e reportando resultados para as partes de interesse.
\end{enumerate}

De acordo com \cite{Fayyad96}, mineração de dados é um passo do processo KDD que consiste na enumeração de padrões sobre os dados, sujeito a aceitáveis limitações de eficiência computacional. Considerando que padrões enumeráveis sobre uma base finita de dados são potencialmente infinitos, e que tal enumeração envolve alguma forma de pesquisa num espaço amplo, restrições computacionais adicionam severos limites sobre o sub-espaço que pode ser explorado por um algoritmo de mineração de dados. Por este motivo, a constante busca por novas metodologias e algoritmos de enumeração de padrões em grandes bases de dados tem sido foco de inúmeras frentes de pesquisa em mineração de dados.

\subsection{Padrões Freqüentes}
\label{sec:introducao_padroes}

De acordo com \cite{Han00}, são chamados padrões freqüentes aqueles que aparecem repetidamente num conjunto de dados. Por exemplo, um par de itens, como ``café'' e ``leite'', que aparecem juntos assiduamente em transações de uma base de dados, é considerado um padrão freqüente. Uma seqüência, como comprar uma câmera fotográfica, e logo depois um cartão de memória, se ocorre muitas vezes na base de dados de um estabelecimento comercial, também é considerada um padrão freqüente.
\par
Padrões freqüentes assumem um papel essencial em muitas tarefas de mineração de dados que possuem, como objetivo, encontrar padrões de determinado interesse numa base, tais como regras de associação, correlações, seqüências, episódios, classificadores, agrupamentos e muitas outras das quais a mineração de regras de associação é uma das mais populares \citep{goethals03survey}. Desde a introdução deste termo, em 1993, por \citeauthor{agrawal93mining}, a mineração de padrões freqüentes e regras de associação têm recebido grande atenção. Nas últimas décadas, centenas de artigos científicos foram publicados apresentando algoritmos, abordagens e melhorias para resolver problemas de mineração de padrões freqüentes e regras de associação de forma mais eficiente.
\par
Dentre as características mais importantes dos padrões freqüentes, podemos citar o suporte, que representa a medida de popularidade do padrão em relação à base. Um padrão é considerado freqüente se possui suporte maior que um dado limite inferior. Na prática, não estamos interessados apenas no conjunto de padrões freqüentes, mas também nos respectivos suportes destes padrões.

\subsection{Regras de Associação}

Em mineração de dados, regras de associação são utilizadas para descobrir elementos que co-ocorrem freqüentemente, sob a relação de causalidade, numa determinada base de dados. A motivação por trás da utilização deste conceito está relacionada com a grande quantidade de aplicações possíveis de se construir com o auxílio de regras de associação, como, por exemplo, questões relacionadas ao comportamento de clientes num determinado estabelecimento comercial.
\par
Considerando uma base de dados de transações em que cada transação represente uma ação de compra de determinado cliente, e cada item da transação represente um produto adquirido, regras de associação seriam uma boa opção para responder questões como:

\begin{itemize}
	\item Encontre todas as regras que possuem ``café'' como conseqüente. Estas regras poderiam auxiliar o usuário a planejar o que deve ser feito no estabelecimento para aumentar a venda daquele produto;
	\item Encontre todas as regras que possuem ``café'' como antecedente. Com estas regras seria possível descobrir que produtos serão impactados se o estabelecimento optar por descontinuar a venda daquele;
	\item Encontre todas as regras que possuem ``café'' como antecedente e ``leite'' como conseqüente. Esta requisição poderia ser realizada para todos os produtos do estabelecimento, com a intenção de se encontrar aqueles cujo consumo esteja relacionado;
	\item Encontre todas as regras relacionando itens localizados nas prateleiras A e B do estabelecimento. Estas regras poderiam auxiliar no planejamento das prateleiras e na organização dos produtos;
	\item Encontre as $k$ ``melhores'' regras que possuem ``café'' como conseqüente. As ``melhores'' regras podem ser obtidas com o auxílio de métricas associadas a cada regra, como suporte e confiança.
\end{itemize}

Uma das características mais difundidas das regras de associação é a confiança. A confiança de uma regra representa a probabilidade de que uma transação da base de dados coberta pelo antecedente da regra também seja coberta pelo termo conseqüente. Em geral, além do conjunto de regras de associação, estamos interessados também no suporte dos termos que as compõem, e na confiança de cada regra.

\section{Ortogonalidade}

Em matemática, ortogonalidade é uma característica que denota perpendicularidade, ou existência de ângulos retos. Formalmente, dois vetores $x$ e $y$ são ortogonais num espaço vetorial $V$ se o produto interno $\left\langle x,y \right\rangle$ é zero. Esta situação é descrita por $x \bot y$.
\par
O termo pode ser estendido para o uso geral, denotando a característica de independência, não redundância, não sobreposição, e, até mesmo, a importância de uma entidade qualquer $Y$ dado que $X$ já é conhecida. Neste trabalho, o termo ortogonalidade está mais relacionado com a ausência de redundância (ou sobreposição), ou seja, o inverso da similaridade.
\par
 Dado um conjunto de elementos, definimos como \textbf{ortogonalidade} a medida do quanto os elementos deste conjunto contribuem com informações não redundantes para a solução de um problema.
\par
Os termos similaridade e ortogonalidade têm sido explorados em várias áreas da Ciência da Computação, e com diversos objetivos. Um exemplo é o caso do \textbf{Modelo de Espaço Vetorial}, aplicado à \textbf{Recuperação de Informação}, que utiliza modelos vetoriais e métricas de similaridade para se aproximar termos da pesquisa de um usuário aos documentos de determinada coleção.
%\par
%De acordo com \cite{salton75vsm}, o modelo de espaço vetorial, ou simplesmente modelo vetorial, representa documentos e consultas como vetores de termos. Aos termos das consultas e documentos são atribuídos pesos que especificam o tamanho e a direção do seu vetor de representação. Estes pesos são utilizados para calcular o grau de similaridade entre cada documento da coleção e a consulta de usuário. Dessa forma, o modelo vetorial leva em consideração documentos que casam com a consulta de forma parcial. Como resultado, o conjunto de respostas é ordenado de forma mais precisa que o antigo e limitado modelo booleano.
\par
Já a ortogonalidade, particularmente, tem sido explorada em áreas como visualização \citep{cui07}, reconhecimento facial \citep{nagao98}, taxonomia \citep{smith00}, dentre outras.
\par
Como exemplo de aplicação do termo em mineração de dados, podemos citar \cite{DBLP:conf/kdd/XinCYH06}, que utiliza métricas de significância e redundância para extrair, de um grande conjunto de padrões freqüentes, um sub-conjunto (top-$k$) de padrões com um valor mínimo de redundância. O autor comenta que este estudo abriu uma nova direção na procura por diferentes e significantes top-$k$ respostas para atividades de mineração de dados, que podem resultar em estudos promissores.

\section{Objetivos}
\label{sec:introducao_objetivos}

Este trabalho visa explorar o problema de classificação associativa em mineração de dados considerando ortogonalidade entre padrões freqüentes com os seguintes objetivos:

\begin{itemize}
	\item Minimizar o número de padrões utilizados na geração das regras, extraindo, do conjunto de padrões freqüentes, um sub-conjunto de padrões ortogonais preservando a mesma qualidade do conjunto original, para que, a partir destes, sejam geradas as regras associativas necessárias para se realizar a classificação;
	\item Diminuir a redundância das regras geradas, como conseqüência da utilização de ortogonalidade aplicada ao conjunto de padrões freqüentes;
	\item Diminuir a ambigüidade das regras geradas, como conseqüência da utilização de ortogonalidade aplicada à cobertura de classes e transações pelos padrões;
	\item Aumentar a efetividade das classificações, como conseqüência da diminuição da redundância e da ambigüidade das regras, de acordo com os itens acima.
\end{itemize}

\section{Trabalhos Relacionados}
\label{sec:introducao_trabalhos}

Esta seção é dedicada ao levantamento de trabalhos relacionados com a proposta da dissertação. Primeiramente, serão apresentados alguns trabalhos relacionados com a diminuição, ou compactação do conjunto de padrões freqüentes obtidos. Em seguida, alguns trabalhos relacionados com a diminuição da redunância no conjunto-resultado, e, por último, trabalhos relacionados com classificação associativa.

\subsection{Diminuição do Conjunto de Padrões Freqüentes}

O problema de compactação do conjunto de padrões freqüentes tem sido abordado de várias formas. Algumas metodologias optam por extrair um sub-conjunto (top-$k$) do conjunto original de padrões freqüentes de forma a maximizar uma determinada função objetivo fornecida pelo usuário, como é o caso encontrado em \cite{DBLP:conf/pkdd/MielikainenM03}. Neste trabalho, os autores utilizam, como critério de seleção dos top-$k$ padrões, a forma cujo sub-conjunto obtido produz melhores estimativas sobre a freqüência dos padrões do conjunto original que não foram selecionados. Em \cite{DBLP:conf/kdd/XinCYH06} é apresentado um trabalho semelhante, onde a função objetivo não está ligada à definição do problema. Dessa forma, um critério de seleção dos top-$k$ padrões pode ser definido de acordo com cada aplicação.
\par
Outras formas de ser compactar o conjunto de padrões freqüentes é desenvolver modelos alternativos de representação de tais conjuntos. Neste caso, o resultado obtido pela aplicação não é, necessáriamente, constituído de um conjunto de padrões. Em \cite{boulicaut03freesets}, por exemplo, os autores introduzem uma estrutura chamada \textit{free-sets} de onde é possível aproximar o suporte de qualquer padrão da base. Os experimentos demonstram que a obtenção de \textit{free-sets} é possível mesmo quando a extração dos padrões freqüentes se torna intratável. Em \cite{DBLP:conf/icde/ZhuYHYC07} são introduzidos os chamados padrões colossais, e uma nova metodologia de mineração de padrões, que aproxima o conjunto resultado do conjunto de padrões freqüentes colossais. Note que, neste caso, o resultado do algoritmo é consituído, não do conjunto de padrões colossais, mas sim de uma aproximação deste conjunto.
\par
Nesta dissertação, estamos interessados em diminuir o tamanho do conjunto-solução como foi feito nos primeiros trabalhos apresentados, ou seja, obtendo, como resultado, um sub-conjunto (top-$k$) de padrões a partir do conjunto original de padrões freqüentes.

\subsection{Diminuição de Redundância no Conjunto de Padrões Freqüentes}

Aqui apresentamos alguns trabalhos relacionados com a obtenção de conjuntos padrões freqüentes com baixa redundância. Em \cite{DBLP:conf/kdd/XinCYH06}, trabalho que já foi citado na seção anterior, os autores apresentam a aplicação do modelo de obtenção de top-$k$ padrões nos problemas de extração de termos de documentos e \textit{prefetch} de blocos em seqüências de acesso ao disco, utilizando uma função objetivo que relaciona duas métricas propóstas - significância e redundância. A idéia é obter, como resultado, um conjunto de padrões com alta significância e baixa redundância, que represente bem todo o conjunto de padrões freqüentes. O artigo apresenta as duas métricas da função objetivo e descreve um algoritmo guloso que resolve o problema com ordem de complexidade de tempo $O(\log k)$.
\par
Em \cite{xin05mining} encontramos uma abordagem diferente para o problema, onde os padrões freqüentes e fechados são agrupados de acordo com uma medida de similaridade baseada em cobertura de transações da base, e, de cada agrupamento, um padrão que possui boa representatividade em relação aos outros elementos do seu conjunto deve ser escolhido e acrescentado à solução. O resultado é um sub-conjunto de padrões freqüentes e fechados com baixa redundância e alta representatividade em relação ao conjunto original.
\par
Em \cite{zaki07origami} encontramos um novo paradigma para obtenção de uma representação resumida do conjunto de padrões freqüentes aplicado ao problema de mineração de grafos. O artigo apresenta a formulação da obtenção do conjunto $\alpha$-ortogonal e $\beta$-representativo como um problema de otimização NP-Hard. Um conjunto de padrões é considerado $\alpha$-ortogonal se a similaridade entre os padrões que o compõem é menor ou igual a um determinado valor $\alpha$. Dado um conjunto de padrões, um sub-conjunto deste é $\beta$-representativo em relação aos elementos que não fazem parte dele se, para cada um destes elementos, existe um elemento no sub-conjunto cuja similaridade entre os dois é maior ou igual a $\beta$. O objetivo é obter sub-conjuntos de padrões não redundantes que possuem um certo nível de representatividade em relação ao conjunto original. Esta abordagem será discutida com mais detalhes na seção \ref{sec:ortogonalidade_origami}.
\par
Nesta dissertação, estamos interessados em minimizar o conjunto de padrões freqüentes com o auxílio de uma função objetivo a ser definida de acordo com a aplicação. No nosso caso, a aplicação utilizada é a classificação associativa, e a função objetivo é a medida de ortogonalidade do conjunto, que está relacionada com a métrica de ortogonalidade utilizada.

\subsection{Classificação Associativa}

Dentre todos os modelos de classificação associativa encontrados na literatura, optamos por utilizar, neste trabalho, um modelo baseado na abordagem \textit{lazy}, introduzida em \cite{Veloso06Lazy}. Este artigo apresenta as vantagens do modelo \textit{lazy} quando comparado com a abordagem \textit{eager}, bastante explorada em árvores de decisão e classificadores baseados em entropia. Em primeiro lugar, artigo demonstra que classificadores associativos \textit{eager} possuem desempenho melhor que árvores de decisão, e discute como regras de decisão podem ser geradas a partir de um modelo baseado em árvores de decisão. Em seguida, os autores apresentam a abordagem \textit{lazy}, e demonstram que ela produz resultados melhores que a abordagem \textit{eager}. Classificadores \textit{eager} geram regras antes que as instâncias de teste sejam analisadas. A dificuldade de se antecipar todas as direções possíveis da tarefa de classificação faz com que a abordagem produza regras muito generalizadas, o que pode reduzir a eficácia do classificador. A estratégia \textit{lazy}, no entanto, só produz regras aplicáveis à instância de teste, o que faz com que a eficácia do classificador não seja penalizado pela generalização. Os resultados apresentados compravam a superioridade da estratégia \textit{lazy}.
\par
Em \cite{veloso06multi}, os autores utilizam a abordagem \textit{lazy} de classificação associativa direcionado para classificação de documentos. O artigo apresenta um algoritmo de classificação capaz de analisar tanto o conteúdo dos documentos quanto a existências de \textit{links} de saída e de entrada para outros documentos. As inovações introduzidas pelos autores produzem resultados mais efetivos e de maior eficácia que abordagens do estado da arte nas coleções utilizadas. A mesma abordagem ainda obteve resultados satisfatórios em outras aplicações \citep{veloso06, Veloso2007}.
\par
No nosso trabalho, aplicamos o conceito de ortogonalidade num modelo de classificação associativa baseada na abordagem \textit{lazy} e direcionado para a classificação de transações em bases de dados genéricas.

%Apesar da compactação do conjunto de padrões freqüentes resultar, em grande parte das ocasiões, na diminuição da redundância do conjunto, são poucos os trabalhos que possuem, como finalidade, soluções que maximizam as duas funções objetivo. Em \cite{pasquier99}, por exemplo, os autores apresentam a definição de padrões fechados, e demonstram que o problema de se encontrar o conjunto de padrões freqüentes numa base de dados pode ser reduzido ao problema de se encontrar o conjunto de padrões freqüentes fechados.
%
%Na literatura, encontramos diversos trabalhos relacionados, tanto com classificação associativa quanto com a obtenção de top-$k$ padrões como forma de compactar o conjunto de padrões freqüentes obtidos de uma base de dados e simplificar a visualização dos resultados, sendo que alguns destes trabalhos também estão relacionados com o termo ortogonalidade.
%\par
%Em \cite{DBLP:conf/kdd/XinCYH06}, os autores introduzem o problema da extração dos top-$k$ padrões livres de redundância através de um modelo que integra duas métricas - significância e relevância - numa única função objetivo. A idéia é obter, a partir do conjunto de todos os padrões freqüentes obtidos por uma abordagem qualquer de mineração de dados, um conjunto de $k$ padrões de alta significância e baixa redundância entre seus elementos. Este conjunto deve ser obtido com o auxílio de uma função objetivo que relaciona as duas métricas propostas. O resultado é um conjunto de padrões não similares que representam bem todo o conjunto de padrões freqüentes. O artigo apresenta duas funções objetivo, e descreve um algoritmo guloso que resolve o problema com ordem de complexidade de tempo $O(\log k)$.
%\par
%Em \cite{Veloso06Lazy} é introduzida a abordagem \textit{lazy} de classificação associativa, apresentando as vantagens deste modelo quando comparado com a abordagem \textit{eager}, bastante explorada em árvores de decisão e classificadores baseados em entropia. Em primeiro lugar, artigo demonstra que classificadores associativos \textit{eager} possuem desempenho melhor que árvores de decisão, e discute como regras de decisão podem ser geradas a partir de um modelo baseado em árvores de decisão. Em seguida, os autores apresentam a abordagem \textit{lazy}, e demonstram que ela produz resultados melhores que a abordagem \textit{eager}. Classificadores \textit{eager} geram regras antes que as instâncias de teste sejam analisadas. A dificuldade de se antecipar todas as direções possíveis da tarefa de classificação faz com que a abordagem produza regras muito generalizadas, o que pode reduzir a eficácia do classificador. A estratégia \textit{lazy}, no entanto, só produz regras aplicáveis à instância de teste, o que faz com que a eficácia do classificador não seja penalizado pela generalização. Os resultados apresentados compravam a superioridade da estratégia \textit{lazy}.
%\par
%Em \cite{veloso06multi}, os autores utilizam a abordagem \textit{lazy} de classificação associativa direcionado para classificação de documentos. O artigo apresenta um algoritmo de classificação capaz de analisar tanto o conteúdo dos documentos quanto a existências de \textit{links} de saída e de entrada para outros documentos. As inovações introduzidas pelos autores produzem resultados mais efetivos e de maior eficácia que abordagens do estado da arte nas coleções utilizadas.
%\par
%Em \cite{zaki07origami} encontramos um novo paradigma para obtenção de uma representação resumida do conjunto de padrões freqüentes. O artigo introduz uma metodologia randômica para obtenção de padrões maximais que possui, como principal característica, a possibilidade de cobrir uniformemente o espaço dos padrões, e ainda apresenta a formulação da obtenção do conjunto $\alpha$-ortogonal e $\beta$-representativo como um problema de otimização NP-Hard. Um conjunto de padrões é considerado $\alpha$-ortogonal se a similaridade entre os padrões que o compõem é menor ou igual a um determinado valor $\alpha$. Dado um conjunto de padrões, um sub-conjunto deste é $\beta$-representativo em relação aos elementos que não fazem parte dele se, para cada um destes elementos, existe um elemento no sub-conjunto cuja similaridade entre os dois é maior ou igual a $\beta$. O objetivo é obter sub-conjuntos de padrões não redundantes que possuem um certo nível de representatividade em relação ao conjunto original. Esta abordagem será discutida com mais detalhes na seção \ref{sec:ortogonalidade_origami}.
%\par
%O problema de compactação do conjunto de padrões freqüentes é explorado de diversas outras maneiras: \cite{TheobaldSW_BTW07} apresenta um \textit{framework} para indexação e recuperação em grandes coleções de dados estruturados, semi-estruturados e não estruturados. Em \cite{DBLP:conf/icde/ZhuYHYC07}, encontramos uma nova metodologia de mineração de padrões, que aproxima o conjunto resultado do conjunto de padrões freqüentes colossais. Em \cite{boulicaut03freesets} os autores introduzem uma estrutura chamada \textit{free-sets} de onde é possível aproximar o suporte de qualquer padrão da base. Os experimentos demonstram que a obtenção de \textit{free-sets} é possível mesmo quando a extração dos padrões freqüentes se torna intratável. Em \cite{fu00} encontramos uma nova forma de se obter o conjunto de padrões freqüentes sem a especificação de um suporte mínimo. Nesta abordagem, o usuário deve simplesmente informar a quantidade de itens que deseja no conjunto-solução. Já em \cite{DBLP:conf/icdm/HanWLT02}, a abordagem de obtenção do conjunto-solução sem especificação de suporte é aplicada à mineração de padrões fechados. Neste caso, o usuário deve informar, além do tamanho desejável do conjunto-solução, o tamanho mínimo dos padrões, e, finalmente, \cite{xin05mining} apresenta o problema de compressão de padrões com o objetivo de encontrar um conjunto de padrões de tamanho mínimo que possua pelo menos um representante para cada padrão freqüente do conjunto real.
%\par
%Também encontramos muitos outros trabalhos relacionados com o problema da classificação: Em \cite{liu98integrating} é apresentada a proposta de integração entre as áreas mineração de regras de associação e classificação baseada em regras. \cite{zaki99charm} demonstra que é possível gerar regras de associação utilizando apenas padrões fechados, sem perda de informação, e apresenta um eficiente algoritmo para mineração destes padrões. Em \cite{DBLP:conf/sdm/WangK05} encontramos um algoritmo de geração de regras com o objetivo de incluir, no conjunto-resultado, apenas as regras de alta confiança encontradas para cada instância da base de teste. Em \cite{li01cmar} os autores apresentam um novo método de classificação associativa baseada em regras de múltipla associação. O algoritmo realiza a classificação de uma instância de teste com o auxílio de um sub-conjunto de regras que casam com a instância. Em \cite{DBLP:conf/sdm/YinH03}, encontramos um algoritmo guloso que gera as regras de associação diretamente da base de treinamento. \cite{kuok98mining} introduz regras de associação baseadas em \textit{fuzzy sets}. Em \cite{DBLP:conf/icde/ChengYHH07} os autores realizam um estudo do problema de classificação e apresentam um \textit{framework} que relaciona padrões freqüentes com métricas discriminativas tais como ganho de informação, e, finalmente, \cite{DBLP:conf/icde/LentSW97} apresenta um algoritmo de classificação \textit{multi-label} com a intenção de reduzir a redundância de informações durante o processo de aprendizado existente nas propostas anteriores.
%\par
%E ainda, a abordagem \textit{lazy} proposta por \cite{Veloso06Lazy}, e utilizada em \cite{veloso06multi}, também obteve resultados satisfatórios em outras aplicações \citep{veloso06, Veloso2007}.

\section{Organização do Documento}

Este documento é dividido em cinco capítulos. Os demais capítulos estão organizados da seguinte forma:

\begin{itemize}
	\item O capítulo \ref{chapter:classificacao} possui uma base de informações relacionadas à classificação associativa, como definição do problema, definição e comparação entre as estratégias \textit{eager} e \textit{lazy}, e uma breve apresentação das métricas mais utilizadas para se comparar regras de associação;
	\item No capítulo \ref{chapter:ortogonalidade} será apresentada a nova estratégia de classificação baseada em ortogonalidade. O conteúdo inclui uma discussão sobre métricas e estratégias de ortogonalidade utilizadas, uma explicação detalhada da utilização de ortogonalidade pelo classificador e das heurística de obtenção de conjuntos ortogonais, e ainda a adaptação da estratégia ORIGAMI (mencionada na seção \ref{sec:introducao_trabalhos}) para o problema de classificação;
	\item No capítulo \ref{chapter:resultados} serão apresentados os experimentos executados e os resultados obtidos durante a realização do trabalho;
	\item O capítulo \ref{chapter:conclusao} possui um breve resumo do que foi apresentado, além de sugestões para futuros trabalhos relacionados ao tema.
\end{itemize}