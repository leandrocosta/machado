\chapter{Padrões Freqüentes e Ortogonais}
\label{chapter:ortogonalidade}

Como já foi mencionado na seção \ref{sec:introducao_padroes}, padrões freqüentes são largamente utilizados em diversas aplicações da mineração de dados, incluindo regras de associação, classificação , agrupamento, indexação, dentre outras. Encontra-se, na literatura, uma grande quantidade de algoritmos de mineração de padrões freqüentes que, para muitas aplicações, produzem resultados satisfatórios (como os apresentados na seção \ref{sec:introducao_trabalhos}). Entretanto, este problema ainda não foi totalmente resolvido.
\par
Uma das razões que contribuem para este fato é que, de acordo com a definição, qualquer sub-conjunto de um padrão freqüente também é freqüente, o que faz com que o tamanho do conjunto-solução do algoritmo cresça de maneira explosiva. A introdução de conceitos como padrões fechados ou padrões maximais ajudou a resolver este problema, porém, as abordagens existentes minimizam o conjunto-solução apenas sob a perspectiva do suporte, não considerando a semântica dos dados, o que pode fazer com que padrões interessantes ao usuário sejam retirados da solução durante o processo de minimização do resultado.
\par
Outro desafio que ainda persiste na mineração de padrões freqüentes é a eliminação de redundância nos resultados obtidos. Em muitas aplicações encontramos a necessidade de extrair um pequeno conjunto de padrões freqüentes que tenham, não só alta significância, mas também baixa redundância. A significância é, em geral, definida pelo contexto da aplicação. De acordo com \cite{DBLP:conf/kdd/XinCYH06}, alguns estudos recentes têm se concentrado em como extrair top-$k$ padrões com alta significância, e outros em como remover redundância entre padrões.
\par
O objetivo da aplicação de ortogonalidade na mineração de padrões freqüentes é desenvolver uma técnica capaz de extrair um sub-conjunto de padrões com alta significância e baixa redundância entre seus elementos.
\par
%Como já foi mencionado no capítulo \ref{chapter:classificacao}, encontra-se, na literatura, diversos trabalhos relacionados com classificação, explorando o problema das mais variadas maneiras, e apresentando modelos diferentes, capazes de se adaptar aos diversos tipos de aplicações existentes.
%\par
%Paralelamente, a ortogonalidade tem aparecido, em alguns dos últimos trabalhos de mineração de dados, como uma forma de se resolver problemas de redundância em conjuntos de padrões freqüentes extraídos de determinada base de dados.
%\par
%A utilização de ortogonalidade para se diminuir a redundância dos conjuntos de padrões utilizados na geração de regras associativas aparece como uma possível forma de se melhorar a eficácia de tais classificadores. Assim, surge a motivação para a aplicação de métricas de ortogonalidade entre padrões no modelo de classificação associativa.
%\par
%Este capítulo apresenta as informações relacionadas à nova abordagem de classificação associativa proposta. Na seção \ref{sec:ortogonalidade_metricas} serão apresentadas as métricas de ortogonalidade exploradas, na seção \ref{sec:ortogonalidade_estrategias} serão discutidas as estratégias de aplicação das métricas. A seção \ref{sec:ortogonalidade_classificacao} possui uma discussão sobre as formas como a ortogonalidade foi utilizada dentro do modelo de classificação, e a seção \ref{sec:ortogonalidade_origami} apresenta a estratégia ORIGAMI e adaptação desenvolvida para o problema de classificação.
 
\section{Métricas de Ortogonalidade}
\label{sec:ortogonalidade_metricas}

Para se extrair um sub-conjunto de padrões de alta significância e baixa redundância do conjunto original de padrões freqüentes, é preciso definir métricas que possibilitem a avaliação dos vários conjuntos-solução possíveis. Em \cite{DBLP:conf/kdd/XinCYH06} os autores utilizam, em seus experimentos, o coeficiente de \textbf{Jaccard} \citep{jain88} aplicado à cobertura da base de dados para se obter uma medida de distância entre dois padrões: \[D(p_1,p_2) = 1 - \frac{|TS(p_1) \cap TS(p_2)|}{|TS(p_1) \cup TS(p_2)|},\] onde $TS(p_1)$ é o conjunto de transações cobertas pelo padrão $p_1$.
\par
Em \cite{zaki07origami} os autores discutem diferentes métricas de similaridade entre grafos, incluindo, além do coeficiente de Jaccard: \[sim(G_a, G_b) = \frac{|t(G_a) \cap t(G_b)|}{|t(G_a) \cup t(G_b)|},\] uma métrica de distância para grafos baseada no máximo sub-grafo comum \citep{bunke98}: \[sim_{mc}(G_1, G_2) = \frac{|G_{mc}|}{max(|G_1|,|G_2|)},\] onde $G_{mc}$ é o maior sub-grafo comum entre $G_1$ e $G_2$.
\par
A principal características destas métricas é que elas foram desenvolvidas para pares de padrões, e não para conjuntos de padrões. No nosso caso, estamos interessados em métricas que definem a ortogonalidade de conjuntos. Nas próximas sub-seções apresentaremos três métricas de ortogonalidade propostas e indicadas para o problema da classificação associativa.

%A aplicação da ortogonalidade no modelo de classificação associativa tem como principal objetivo a diminuição de redundância no conjunto de regras geradas durante a fase de treinamento. Para tanto, podem ser  naO objetivo da ortogonalidade é diminuir o número de padrões utilizados para gerar as regras de associação. Logo, a sua utilização ....

\subsection{Similaridade entre Padrões}
\label{sec:ortogonalidade_metricas_similaridade}

Considerando a estrutura dos padrões, dizemos que dois padrões são ortogonais se eles não possuem itens em comum. Dessa forma, pode-se dizer que os padrões $ABC$ e $DEF$ são ortogonais, mas $ABC$ e $CDE$ não são, já que o item $C$ está presente nos dois padrões. O mesmo pode ser aplicado para conjuntos maiores de padrões, por exemplo, os padrões $AB$, $CD$ e $EF$ são ortogonais, mas os padrões $AB$, $BC$ e $CD$ não são.
\par
Entretanto, a simples informação de um determinado conjunto de padrões é ortogonal ou não é insuficiente para que a possível solução seja avaliada. É preciso desenvolver uma métrica que forneça uma medida contínua de ortogonalidade.
\par
Uma forma de se obter esta medida, é analisar os itens dos padrões. Sabemos que dois padrões são ortogonais se eles não compartilham itens, ou seja, a presença de um mesmo item em mais de um padrão do conjunto deve diminuir a medida de ortogonalidade do mesmo.
\par
Assim, para cada item $i$, pertencente a, pelo menos, um dos padrões do conjunto, é dado um peso \[w_i = \frac{p-k}{p-1},\] onde $k$ é o número de padrões do conjunto que contém o item $i$. De acordo com esta expressão, se o conjunto possui quatro padrões, e um item $i$ está presente em apenas um deles, o seu peso $w_i$ é $1$. Se o item está presente em $3$ padrões do conjunto, o peso $w_i$ é $\frac{1}{3}$. Itens compartilhados por todos os padrões têm peso igual a $0$.
\par
Considerando que existam $I$ itens distintos em todo o conjunto de padrões, a ortogonalidade do conjunto é dada pela expressão: \[\frac{\sum_i w_i}{I}.\]
\par
TODO: inserir um exemplo aqui.
%Dois padrões são similares se possuem itens em comum, logo, dois padrões são ortogonais se não possuem itens em comum. Mostrar exemplo: AB e CD são mais ortogonais que AB e BC. Fazer figura do exemplo.

\subsection{Cobertura de Transações}
\label{sec:ortogonalidade_metricas_transacoes}

Considerando o espaço de transações, dizemos que dois padrões são ortogonais se eles cobrem áreas diferentes da base de dados, ou seja, se não existem sobreposições nas coberturas de cada padrão do conjunto.
\par
Na figura \ref{fig:covex1} temos uma representação de uma base de dados, e os conjuntos de transações cobertas por três padrões ($X$, $Y$ e  $Z$). As transações que fazem parte das áreas $T_X$, $T_Y$ e $T_Z$ são cobertas por apenas um dos padrões ($X$, $Y$ e $Z$ respectviamente). As transações de $T_{XY}$ são cobertas por $X$ e $Y$, e as transações de $T_{XYZ}$ são cobertas pelos três padrões do exemplo. Se precisamos de padrões que cobrem diferentes áreas da base de dados, então nós estamos interessados em maximizar $T_X$, $T_Y$ e $T_Z$, neste caso.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{img/coverage}
\caption{Orthogonality by Coverage}
\label{fig:covex1}
\end{figure}

A métrica de cobertura de transações pode ser calculada de maneira análoga à relacionada métrica de similaridade entre padrões, sendo que, neste caso, os elementos que possuem pesos a serem calculados são as transações cobertas pelos padões, e não os seus itens.
\par
Consideramos que cada transação $t$, coberta por, pelo menos, um dos $p$ padrões do conjunto, possui um peso, e este peso é dado por: \[w_t = \frac{p - k}{p - 1},\] onde $k$ é o número de padrões que cobrem a transação $t$. De acordo com esta expressão, se nós temos 4 padrões, e uma transação $t$ é coberta por apenas um deles, então o seu peso $w_t$ será $1$. Se esta transação é coberta por dois padrões do conjunto, o valor de $w_t$ será $\frac{1}{2}$. As transações cobertas por todos os padrões do conjunto terão peso igual a $0$.
\par
Dada esta expressão para se calcular o peso de cada transação, podemos, agora, calcular a medida de ortogonalidade do conjunto de padrões. Considerando que temos $T$ transações distintas e cobertas pelos padrões, a medida de ortogonalidade do conjunto é dada por: \[\frac{\sum_t w_t}{T}.\]
\par
TODO: inserir um exemplo aqui.

\subsection{Cobertura de Classes}
\label{sec:ortogonalidade_metricas_classes}

As duas métricas de ortogonalidade apresentadas nas seções \ref{sec:ortogonalidade_metricas_similaridade} e \ref{sec:ortogonalidade_metricas_transacoes} estão relacionadas apenas com os padrões e as transações da base, e podem ser utilizadas em qualquer tipo de aplicação. Já a métrica apresentada nesta seção é voltada diretamente para o problema da classificação, pois está relacionada às classes das transações cobertas pelos padrões do conjunto.
\par
Dois padrões são ortogonais se são encontrados em transações de classes distintas na base de dados, ou seja, o conjunto de transações cobertas por cada um dos padrões não devem possuir classes em comum.
\par
A métrica de cobertura de classes pode ser calculada de maneira análoga às relacionadas métricas de similaridade entre padrões e cobertura de transações, sendo que, neste caso, os elementos que possuem pesos a serem calculados são as classes existentes nas transações cobertas pelos padrões.
\par
Consideramos que cada classe $c$, que aparece em transações cobertas por, pelo menos, um dos $p$ padrões do conjunto acima de $90\%$ da média esperada, possui um peso, e este peso é dado por: \[w_c = \frac{p - k}{p - 1},\] onde $k$ é o número de padrões que cobrem uma quantidade de transações da classe $c$ maior que o esperado. De acordo com esta expressão, se nós temos 4 padrões, e uma classe $c$ que aparece em transações cobertas por apenas um deles, então o seu peso $w_c$ será $1$. Se esta classe aparece em transações coberta por dois padrões do conjunto de maneira balanceada (por exemplo, se a classe aparece em $50\%$ das transações cobertas por cada padrão), o valor de $w_c$ será $\frac{1}{2}$. As classes que aparecem por igual em transações cobertas por todos os padrões do conjunto terão peso igual a $0$.
\par
Dada esta expressão para se calcular o peso de cada classe, podemos, agora, calcular a medida de ortogonalidade do conjunto de padrões. Considerando que temos $C$ classes distintas que aparecem em transações cobertas pelos padrões, a medida de ortogonalidade do conjunto é dada por: \[\frac{\sum_c w_c}{C}.\]
\par
TODO: pensar num exemplo com figura.

\section{Estratégias de Ortogonalidade}
\label{sec:ortogonalidade_estrategias}

Após a introdução das métricas, é necessário definir como elas serão utilizadas para se obter a medida de ortogonalidade do conjunto. Na literatura, encontramos algumas estratégias interessantes.
\par
Em \cite{zaki07origami}, a métrica de ortogonalidade só é aplicada a pares de padrões, e os autores optaram por utilizar como valor de ortogonalidade do conjunto a medida da ortogonalidade entre os dois elementos mais similares deste conjunto.
\par
Em \cite{DBLP:conf/kdd/XinCYH06} os autores optaram por construir métricas de significância aplicáveis a padrões e métricas de redundância aplicáveis a pares de padrões. O valor da função que compreende as duas métricas é obtido pela soma das significâncias de cada padrão do conjunto menos a média das redundâncias existentes entre cada par de padrões.
\par
Neste trabalho, duas soluções foram implementadas. A primeira delas é a ortogonalidade por conjunto, onde aplicamos as métricas de ortogonalidade apresentadas na seção \ref{sec:ortogonalidade_metricas} considerando todo o conjunto solução. A segunda é baseada na solução proposta por \cite{DBLP:conf/kdd/XinCYH06} - as métricas são aplicadas somente a pares de padrões, e a ortogonalidade do conjunto é obtida por meio da média das ortogonalidades de cada par.
\par
É fácil perceber que as três métricas propostas na seção \ref{sec:ortogonalidade_metricas} são equivalentes ao coeficiente de Jaccard quando aplicadas a conjuntos de dois padrões. Tomando, como exemplo, a métrica de similaridade entre padrões, temos que a ortogonalidade do conjunto é dada pela expressão: \[\frac{\sum_i(\frac{p-k}{p-1})}{I}.\] Nos casos em que os conjuntos possuem necessariamente dois padrões: \[\frac{\sum_i(2-k)}{I},\] onde $k$ é o número de padrões que possuem o item $i$.
\par
Analisando a expressão acima, temos que $I$ corresponde ao tamanho do conjunto de itens encontrados nos dois padrões, ou seja, \[I = |p_1 \cup p_2|,\] onde $|p_1|$ é o tamanho do padrão $p_1$, e a parcela $(2-k)$ terá valor $1$ para itens que estão presentes em apenas um padrão, e $2$ para itens presentes nos dois padrões, ou seja, \[\sum_i(2-k) = |p_1 \cap p_2|,\] logo, a métrica de ortogonalidade baseada em similaridade entre padrões, quando aplicada par-a-par, é dada pela seguinte expressão: \[\frac{\sum_{i=1}^{N-1}{\sum_{j=i+1}^N{\frac{|p_i \cap p_j|}{|p_i \cup p_j|}}}}{\frac{N \times (N-1)}{2}},\] onde $N$ é o número de padrões do conjunto.
\par
A métrica de ortogonalidade baseada em cobertura de transações, quando aplicada par-a-par, é dada pela seguinte expressão: \[\frac{\sum_{i=1}^{N-1}{\sum_{j=i+1}^N{\frac{|TS(p_i) \cap TS(p_j)|}{|TS(p_i) \cup TS(p_j)|}}}}{\frac{N \times (N-1)}{2}},\] onde $N$ é o número de padrões do conjunto, e $TS(p_i)$ é o conjunto de transações cobertas pelo padrão $p_i$.
\par
A métrica de ortogonalidade baseada em cobertura de classes, quando aplicada par-a-par, é dada pela seguinte expressão: \[\frac{\sum_{i=1}^{{N_p}-1}{\sum_{j=i+1}^{N_p}{\left(\frac{\sum_{k=1}^{N_c}{\frac{|CS(p_i) - CS(p_j)|}{max(|CS(p_i)|, |CS(p_j)|)}}}{N_c}\right)}}}{\frac{N \times (N-1)}{2}},\] onde $N_p$ é o número de padrões do conjunto, $N_c$ é o número de classes cobertas pelos padrões e $CS(p_i)$ é o conjunto de classes cobertas pelo padrão $p_i$.
%Breve introdução, falar sobre métricas utilizadas por \cite{DBLP:conf/kdd/XinCYH06} e \cite{zaki07origami}.

%\subsection{Ortogonalidade por conjunto}

%Apresentar e discutir as expressões: \\
%Similaridade: Expressão que dá um peso para cada item encontrado nos padrões do conjunto; \\
%Cobertura de Transações: Expressão que dá um peso para cada transação coberta pelo conjunto de padrões; \\
%Cobertura de Classes: Expressão que dá um peso para cada classe encontrada nas transações cobertas pelos padrões.

%\subsection{Ortogonalidade Par-a-par}

%Discutir as expressões par a par (similaridade pode ser dada por interseção sobre união de elementos, ortogonalidade pode ser dada por 1 - similaridade): \\
%Ortogonalidade por itens: Quantidade de itens que aparecem em apenas um dos padrões / quantidade de itens que aparecem nos dois padrões. \\
%Ortogonalidade por cobertura de transações: Quantidade de transações cobertas por apenas um dos padrões / quantidade de transações cobertas pelos dois padrões. \\
%Ortogonalidade por cobertura de classes: Mais complicado: Ortogonalidade é a média das razões entre a diferença das coberturas e a maior cobertura de cada classe. 

\section{Classificação e Ortogonalidade}
\label{sec:ortogonalidade_classificacao}

Como já foi dito na seção \ref{sec:introducao_objetivos}, a utilização da ortogonalidade no problema da classificação associativa tem como objetivo aumentar a efetividade das classificações, como conseqüência da diminuição da redundância e da ambigüidade das regras. Uma das formas de se fazer isso seria aplicar ortogonalidade no conjunto de regras geradas por classificadores associativos. Com isso, seria possível extrair, de todo o conjunto de regras obtidas, apenas um sub-conjunto representativo de regras com alta significância e baixa redundância, e a partir destas, realizar a classificação das instâncias de teste.
\par
Em \cite{DBLP:conf/kdd/XinCYH06} os autores apresentam uma proposta de utilização de ortogonalidade aplicada a \textit{prefetch} de blocos de acesso a disco. Os autores consideram a similaridade de duas regras igual a $0$ quando os termos conseqüentes são diferentes, e igual ao coeficiente de Jaccard aplicado aos termos antecedentes quando os conseqüentes são iguais.
\par
É possível aplicar este modelo no problema da classificação associativa, considerando a ortogonalidade entre duas regras é igual a $0$ se as classes para as quais elas apontam são diferentes, e inversamente proporcionais ao coeficiente de Jaccard (ou alguma outra métrica qualquer) aplicado aos termos antecedentes se as classes para as quais elas apontam são iguais. Dessa forma, estaríamos gerando todas as regras possíveis, a partir do conjunto de padrões freqüentes obtidos por meio de um determinado suporte, e extraindo um sub-conjunto que consiste apenas das mais representativas, para então realizar a classificação da instância de teste.
\par
Neste trabalho, porém, optamos por aplicar a ortogonalidade não ao conjunto de regras geradas, mas sim ao conjunto de padrões utilizados para gerar as regras. A idéia é gerar regras a partir de um sub-conjunto dos padrões freqüentes obtidos, que consiste apenas dos padrões ortogonais. A principal diferença desta abordagem, em relação à anterior, é que as métricas de ortogonalidade são aplicadas apenas aos termos antecedentes - os padrões, enquanto que, na abordagem anterior, também são considerados os termos conseqüentes das regras, ou seja, as classes.
\par
As três métricas descritas na seção \ref{sec:ortogonalidade_metricas} foram utilizadas, cada uma com uma intenção. A similaridade entre padrões foi utilizada com o objetivo de se encontrar um sub-conjunto de padrões represente bem todo o conjunto de padrões freqüentes. Por ser uma métrica que considera a representação estrutural dos padrões, ela é uma métrica que se aplica no espaço dos padrões, e não das transações da base. Neste caso, estamos interessados em diminuir, principalmente, a redundância das regras, ou seja, não gerar regras com um alto nível de similaridade entre os termos antecedentes.
\par
A cobertura de transações foi utilizada com o objetivo de se encontrar padrões ortogonais no espaço de transações. Com esta métrica, espera-se obter um conjunto de padrões cuja cobertura da base de dados seja ortogonal, ou seja, que as regras geradas por estes padrões apontem para transações diferentes na base de dados. A intenção, neste caso, é diminuir a ambigüidade das regras, além da redundância.
\par
A cobertura de classes é uma métrica definida especialmente para a classificação, já que ela considera as classes das transações da base de treinamento para definir o conjunto de padrões ortogonais. Esta métrica é uma adaptação da cobertura de transações, já que ela também está centrada na base de dados. A diferença é que, ao contrário da cobertura de transações, esta métrica analisa as classes, e não as transações cobertas pelos padrões. A motivação é que não basta extrair padrões que cobrem transações distintas da base de dados se estas transações classes coincidentes. Esta métrica tenta garantir que as regras geradas pelo conjunto-resultado de padrões apontem para classes distintas. A intenção, como na métrica anterior, é diminuir a ambigüidade das regras, além da redundância.

%Falar sobre como utilizar o conceito de ortogonalidade em algoritmos de regras de associação:
%Ortogonalidade por itens: Motivação: Encontrar um conjunto de padrões ortogonais que representem bem todo o conjunto de padrões freqüentes, e que gere regras não redundantes;
%Ortogonalidade por cobertura de transações: Encontrar um conjunto de padrões que cubram áres ortogonais, e dessa forma, distintas da base de dados, diminuindo a redundância de informações durante a geração das regras, e mais que isso, diminuindo a possibilidade de encontrar regras redundantes;
%Ortogonalidade por cobertura de classes: Encontrar um conjunto de padrões que aparecem em transações de classes distintas, ou seja, padrões que apontam para classes distintas. A idéia é gerar regras não redundantes, e obter a classificação das regras de maior peso no ranking.

\subsection{Utilização de Ortogonalidade no LAC}

O \textbf{LAC} (\textit{Lazy Associative Classifier}) é uma implementação da abordagem \textit{lazy} proposta em \cite{Veloso06Lazy}. A abordagem de classificação associativa \textit{lazy} obtém o conjunto de regras de associação relacionadas a cada instância de teste separadamente. Para tanto, ela cria uma projeção da base de treinamento apenas com as transações que possuem itens em comum com a instância de teste. A partir desta projeção, a abordagem obtém um conjunto de padrões freqüentes, de acordo com determinado suporte fornecido pelo usuário, e com estes padrões, gera as regras de associação utilizadas durante a tarefa de classificação.
\par
A ortogonalidade pode ser utilizada de várias maneiras no \textit{lazy}, por exemplo, é possível extrair, \textit{apriori}, o conjunto de itens ortogonais da base de treinamento, considerando cobertura de transações, ou cobertura de classes, e, para cada instância de testes, obter a projeção da base de treinamento considerando apenas os itens ortogonais encontrados na instância de teste. Esta seria uma boa opção para diminuir o espaço de busca durante a obtenção do conjunto de padrões freqüentes nos casos em que as bases de dados são muito densas.
\par
Uma outra forma de aplicar a ortogonalidade seria extrair o conjunto de itens ortogonais, não de toda a base de treinamento, mas sim da instância no momento do teste. Entretanto, com estas duas escolhas ainda seria possível gerar conjuntos de regras redundantes, já que o conjunto de itens extraídos, mesmo que mantendo a característica de ortogonalidade entre si, poderiam gerar padrões similares.
\par
Sendo assim, a forma escolhida neste trabalho foi continuar utilizando todos os itens para gerar a projeção, obter o conjunto de padrões freqüentes e, deste conjunto, obter o sub-conjunto de padrões ortogonais, com o qual são geradas as regras de associação.

\subsection{Heurística de Obtenção de Conjuntos Ortogonais}

O problema de se encontrar o sub-conjunto de padrões com maior métrica de ortogonalidade, dado o conjunto de padrões freqüentes, é \textbf{NP}, visto que todas as combinações de todos os tamanhos possíveis devem ser testadas para se chegar ao resultado final. Portanto, foi desenvolvida uma heurística gulosa que obtém um conjunto ortogonal de dois elementos, e iterativamente, tenta obter um novo conjunto com um elemento a mais, acrescentando um novo padrão e realizando modificações par que a métrica de ortogonalidade seja maximizada.
\par
Esta abordagem é semelhante ao que foi proposto em \cite{zaki07origami}. Este artigo apresenta um algoritmo que considera o conjunto de padrões freqüentes como um grafo em que cada vértice representa um padrão, uma aresta entre dois padrões representa a similaridade entre eles. No entanto, só são representadas as arestas que possuem similaridade menor que $\alpha$. O objetivo do algoritmo é encontrar um clique de tamanho máximo neste grafo. Para tanto, o algoritmo escolhe um vértice aleatoriamente e o adiciona no conjunto-solução. Depois disso, o algoritmo passa a visitar os vizinhos dos vértices que já fazem parte da solução, escolhendo sempre a melhor opção para adicionar ao conjunto, até que não haja mais vértices para se adicionar.
\par
No nosso caso, todos os padrões são candidatos ao conjunto-resultado, portanto, a modelagem em grafo é inadequada. A obtenção do conjunto ortogonal de padrões é realizada de forma iterativa, onde no início da execução, o algoritmo inicializa o conjunto-solução com apenas um elemento, e a métrica de ortogonalidade do conjunto com o valor $0$ (zero), e então começa o ciclo de iterações em que, a cada etapa:

\begin{enumerate}
	\item Um novo elemento é incluído ao conjunto;
	\item É realizada uma busca por todo o conjunto de padrões que não fazem parte do conjunto-solução. Durante esta busca, cada padrão verificado é incluído na solução substituindo o padrão deste conjunto mais semelhante àquele. Se a métrica de ortogonalidade do conjunto melhorou, o algoritmo mantém a troca. Se não, a troca é desfeita, e o próximo padrão da seqüência é verificado;
	\item Ao final do processo, o algoritmo compara a métrica de ortogonalidade obtida com a métrica do conjunto anterior (que possuía um elemento a menos). Se a métrica se manteve, ou melhorou, o algoritmo volta ao início do ciclo. Se não, o algoritmo termina o ciclo, e o conjunto anterior é tido como resultado.
\end{enumerate}

\begin{enumerate}
	\item O conjunto de padrões é ordenado de forma decrescente quanto ao tamanho dos padrões;
	\item O primeiro padrão é adicionado ao conjunto solução;
	\item Um novo padrão é adicionado ao conjunto;
	\item Todo o conjunto de padrões que ainda não fazem parte do conjunto solução é percorrido, na tentativa de se melhorar a solução;
	\item Ao final da lista, se a solução não piorou, o conjunto se mantém, e o algoritmo tenta obter um conjunto maior. Se a solução piorou, o algoritmo para, e a solução anterior é retornada.
\end{enumerate}

A ordem como o algoritmo percorre o conjunto de padrões em cada iteração possui uma grande influência no algoritmo, pois um novo padrão verificado só fará parte do conjunto solução se a ortogonalidade do novo conjunto for maior que a do antigo, logo, em caso de empate, os primeiros padrões terão preferência sobre os últimos. Cinco formas diferentes de ordenação do conjunto de padrões freqüentes foram implementadas:

\begin{enumerate}
	\item Ordem lexicográfica crescente;
	\item Ordem lexicográfica descrescente;
	\item Ordem de tamanho crescente;
	\item Ordem de tamanho decrescente;
	\item Nenhuma ordenação.
\end{enumerate}

\section{Estratégia ORIGAMI}
\label{sec:ortogonalidade_origami}

O \textbf{ORIGAMI} é um algoritmo para mineração de grafos apresentado em \cite{zaki07origami}. Neste artigo, os autores introduzem a definição de conjuntos $\alpha$-ortogonais e $\beta$-representativos, e apresentam o novo paradigma de mineração de conjuntos de grafos ortogonais com foco nos padrões, e não nas transações.

\subsection{Definição de alfa-ortogonalidade}
\label{sec:ortogonalidade_origami_definicao}

Seja $\mathcal{F}$ o conjunto de todos os sub-grafos freqüentes de uma coleção. Seja $sim : \mathcal{F} \times \mathcal{F} \rightarrow \left[0, 1\right]$ uma função binária e simétrica que retorna a \textit{similaridade} entre dois grafos, por exemplo, a similaridade entre dois grafos $G_a$ e $G_b$ baseada no máximo sub-grafo comum \citep{bunke98} é dada por \[sim(G_a, G_b) = \frac{|G_c|}{max(|G_a|, |G_b|)},\] onde $G_c$ é o máximo sub-grafo comum entre $G_a$ e $G_b$.
\par
Dada uma coleção de grafos $\mathcal{G}$, e um limite superior para similaridade $\alpha \in \left[0, 1\right]$, dizemos que o sub-conjunto de grafos $\mathcal{R} \subseteq \mathcal{G}$ é \textbf{$\alpha$-ortogonal} em relação a $\mathcal{G}$ se, e somente se, para quaisquer $G_a, G_b \in \mathcal{R}, sim(G_a, G_b) \leq \alpha$ e para qualquer $G_a \in \mathcal{R}$ e qualquer $G_b \in \mathcal{G} \backslash \mathcal{R}, sim(G_a, G_b) > \alpha$.
\par
Dada uma coleção de grafos $\mathcal{G}$,um conjunto $\alpha$-ortogonal $\mathcal{R} \subseteq \mathcal{G}$ e um dado limite inferior para similaridade $\beta \in \left[0, 1\right]$, dizemos que $\mathcal{R}$ \textbf{representa} um grafo $G \in \mathcal{G}$ se existe algum $G_a \in \mathcal{R}$ tal que $sim(G_a, G) \geq \beta$. Seja $\Upsilon(\mathcal{R},\mathcal{G}) = \left\{G \in \mathcal{G} : \exists G_a \in \mathcal{R}, sim(G, G_a) \geq \beta\right\}$, dizemos que $\mathcal{R}$ é um conjunto $\beta$-representativo para $\Upsilon(\mathcal{R}, \mathcal{G})$.
\par
Dada uma coleção de grafos $\mathcal{G}$ e o seu conjunto $\alpha$-ortogonal e $\beta$-representativo $\mathcal{R}$, chamamos de \textbf{conjunto resíduo} de $\mathcal{R}$ o conjunto de padrões não representados em $\mathcal{G}$, dado como $\Delta(\mathcal{R}, \mathcal{G}) = \mathcal{G} \backslash \left\{ \mathcal{R} \cup \Upsilon(\mathcal{R}, \mathcal{G}) \right\}$, o \textit{resíduo} de $\mathcal{R}$ é definido como a cardinalidade do seu conjunto resíduo $|\Delta(\mathcal{R}, \mathcal{G})|$. Finalmente, definimos a média de similaridade do resíduo de $\mathcal{R}$ como $ars(\mathcal{R}, \mathcal{G}) = \frac{\sum_{G_b \in \Delta(\mathcal{R}, \mathcal{G})} {max_{G_a \in \mathcal{R}} \left\{sim(G_a, G_b)\right\}}}{|\Delta(\mathcal{R}, \mathcal{G})|}$.
\par
Note que, por definição, $\alpha < ars(\mathcal{R}, \mathcal{G}) < \beta$, já que para quaisquer $G_a \in \mathcal{R}$ e $G_b \in \Delta(\mathcal{R}, \mathcal{G}), sim(G_a, G_b) \in \left(\alpha, \beta \right)$.
\par
O objetivo dos autores é encontrar conjuntos de grafos $\alpha$-ortogonais e $\beta$-representativos em relação ao conjunto de sub-grafos maximais $\mathcal{M}$. Como o conjunto de padrões maximais provê uma síntese de todos os padrões freqüentes, e com uma quantidade bem menor de padrões, parece razoável tentar um conjunto representativo ortogonal em relação àquele. Entretanto, como encontrar todos os sub-grafos maximais em aplicações reais pode se tornar um problema intratável, os autores optaram por utilizar um sub-conjunto do conjunto de sub-grafos maximais $\widehat{\mathcal{M}} \subseteq \mathcal{M}$. Logo, o problema pode ser definido da seguinte forma:

\begin{problem}[Mineração de grafos $\alpha$-ortogonais e $\beta$-representativos]
Dado um sub-conjunto $\mathcal{\widehat{\mathcal{M}}}$ do conjunto de grafos maximais $\mathcal{\mathcal{M}}$ de uma coleção de grafos $\mathcal{G}$, um limite superior para similaridade $\alpha$ e um limite inferior para similaridade $\beta$, encontre o melhor sub-conjunto $\mathcal{R}$ que minimize o resíduo $|\Delta(\mathcal{R}, \mathcal{\widehat{\mathcal{M}}})|$.
\end{problem}

\subsection{Estratégia de Ortogonalidade}

A utilização dos parâmetros adicionais $\alpha$ e $\beta$ pelo ORIGAMI enriquece o modelo de conjuntos ortogonais. O parâmetro $\alpha$ permite que a medida da ortogonalidade seja controlada, fazendo com que o conjunto-solução se aproxime ou se distancie do conjunto de padrões freqüentes de acordo com o valor escolhido para $\alpha$. Já o parâmetro $\beta$ permite medir a representatividade do conjunto em relação ao restante dos elementos que não fazem parte da solução.
\par
Dependendo dos valores escolhidos para os dois parâmetros duas variantes do problema são identificadas:

\begin{itemize}
	\item{Caso I ($\beta \leq \alpha$):} Pela definição de conjunto $\alpha$-ortogonal, $G_a \in \mathcal{R}$ e $G_b \in \widehat{\mathcal{M}} \backslash \mathcal{R}$ implica em $sim(G_a, G_b) > \alpha \geq \beta$. Logo, temos que $\Upsilon(\mathcal{R}, \mathcal{\widehat{M}}) = \widehat{\mathcal{M}} \backslash \mathcal{R}$, de onde temos que $\Delta(\mathcal{R}, \mathcal{\widehat{M}}) = 0$. Então, quando $\beta \leq \alpha$, o resíduo de qualquer conjunto $\alpha$-ortogonal $\mathcal{R}$ é $0$, o que implica que qualquer conjunto $\alpha$-ortogonal é ótimo, considerando o resíduo;
	\item{Caso II ($\beta > \alpha$):} Este é o caso geral, onde um conjunto $\alpha$-ortogonal $\mathcal{R}$ pode não ser um conjunto $\beta$-representativo para alguns grafos em $\widehat{\mathcal{M}}$. Em outras palavras, quando $\beta > \alpha$, o resíduo $|\Delta(\mathcal{R}, \widehat{\mathcal{M}})| \geq 0$; logo, a solução ótima é o conjunto de padrões ortogonais que minimiza o resíduo. Um caso especial de $\beta > \alpha$ ocorre quando $\beta = 1$. Neste caso, cada elemento do conjunto $\alpha$-ortogonal representa somente a si mesmo, e o resíduo é dado por $|\Delta(\mathcal{R}, \widehat{\mathcal{M}})| = |\widehat{\mathcal{M}} \backslash \mathcal{R}|$.
\end{itemize}

O algoritmo ORIGAMI realiza a mineração de padrões ortogonais em dois passos distintos. O primeiro passo consiste em encontrar, utilizando uma heurística randômica, um sub-conjunto dos padrões maximais da base de dados. O segundo consiste em obter, novamente com o auxílio de uma heurística randômica, um conjunto ortogonal e representativo que minimize o resíduo. O pseudo-código do ORIGAMI pode ser visto no algoritmo \ref{alg:origami}.

\begin{algorithm}
\caption{ORIGAMI}
\label{alg:origami}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{D}, \sigma, \alpha, \beta$
\STATE $EM \leftarrow EdgeMap (\mathcal{D})$
\STATE $\mathcal{F}_1 \leftarrow FindFrequentEdges (\mathcal{D}, \sigma)$
\STATE $\widehat{\mathcal{M}} \leftarrow 0$
\WHILE {$\neg StopCondition ()$}
	\STATE $M \leftarrow RandomMaximalGraph (\mathcal{D}, \mathcal{F}_1, EM, \sigma)$
	\STATE $\widehat{\mathcal{M}} \leftarrow \widehat{\mathcal{M}} \cup M$
\ENDWHILE
\STATE $\mathcal{R} \leftarrow OrthogonalRepresentativeSets (\widehat{\mathcal{M}}, \alpha, \beta)$

\end{algorithmic}
\end{algorithm}

%  , permitindo que a característica do conjunto ortogonal seja controlada. O parâmetr $\alpha$ permite
%Falar das vantagens de alfa e beta.
%Falar da estratégia aleátória, das métricas, mostrar o algoritmo.

\subsection{Implementação}

Foi realizada a implementação do algoritmo ORIGAMI adaptado ao problema de classificação associativa utilizando as métricas apresentadas na seção \ref{sec:ortogonalidade_metricas}.
\par
Como heurística de obtenção do conjunto de padrões maximais, o algoritmo inicia a execução com o conjunto-resultado vazio e, a cada iteração, tenta obter o maior padrão freqüente possível adicionando a ele, aleatoriamente, itens que fazem parte da instância de teste, até que não seja mais possível adicionar nenhum novo item, ou a condição de parada local seja atingida. Se, durante a obtenção aleatória dos itens, o item selecionado já ter sido utilizado, ou não gerar um novo padrão freqüente, o algoritmo decrementa um contador de tentativas. A condição de parada local para a geração de novos padrões maximais é que, durante este processo, o número máximo de escolhas erradas dos itens é igual ao tamanho da instância de teste.
\par
Ao obter um novo padrão maximal, o algoritmo tenta inseri-lo no conjunto-solução. Esta operação consiste em remover do conjunto todos os sub-padrões do novo candidato, e inserir o candidato caso nenhum dos padrões que ainda existem na solução seja super-padrão dele. A condição de parada para o algoritmo é que, durante todo o processo, o número máximo de padrões candidatos não maximais, ou já inseridos no conjunto-solução gerados é igual ao tamanho da instância de teste.
\par
Como heurística para obtenção do conjunto ortogonal, o algoritmo inicia a execução com o valor de resíduo igual a $0$ (zero) e, a cada iteração, tenta obter um conjunto ortogonal adicionando a ele, aleatoriamente, padrões maximais obtidos no primeiro passo do algoritmo, até que não seja mais possível acrescentar novos padrões, ou a condição de parada local seja atingida. Se, durante a obtenção dos padrões, o padrão selecionado já ter sido utilizado, ou não possuir similaridade menor que $\alpha$ para com todos os outros padrões do conjunto-solução, o algoritmo decrementa um contador de tentativas. A condição de parada local para a geração de conjuntos ortogonais é que, durante este processo, o número máximo de escolhas erradas de padrões não pode ser maior que a quantidade de padrões maximais total.
\par
Ao obter um novo conjunto ortogonal, o algoritmo calcula o valor do seu resíduo. Se este valor é menor que o atual, o resíduo é atualizado, e o conjunto-solução passar a ser o conjunto ortogonal recém-encontrado. A condição de parada para o algoritmo é que, durante todo o processo, o número máximo de conjuntos ortogonais candidatos que não melhoram o resultado não pode ser maior que a quantidade de padrões maximais total.