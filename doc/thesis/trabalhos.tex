\chapter{Trabalhos Relacionados}
\label{chapter:trabalhos}

Este capítulo é dedicado ao levantamento de trabalhos relacionados com a proposta da dissertação. Primeiramente, serão apresentados alguns trabalhos relacionados com a diminuição, ou compactação do conjunto de padrões freqüentes obtidos. Em seguida, alguns trabalhos relacionados com a diminuição da redundância no conjunto-resultado, e, por último, trabalhos relacionados com classificação associativa.

\section{Diminuição do Conjunto de Padrões Freqüentes}

O problema de compactação do conjunto de padrões freqüentes tem sido abordado de várias formas. Algumas metodologias optam por extrair um sub-conjunto (top-$k$) do conjunto original de padrões freqüentes de forma a maximizar uma determinada função objetivo fornecida pelo usuário, como é o caso encontrado em \cite{DBLP:conf/pkdd/MielikainenM03}. Neste trabalho, os autores utilizam, como critério de seleção dos top-$k$ padrões, uma função cujo sub-conjunto obtido produz as melhores estimativas sobre a freqüência dos padrões do conjunto original que não foram selecionados. Em \cite{DBLP:conf/kdd/XinCYH06} é apresentado um trabalho semelhante, onde a função objetivo não está ligada à definição do problema. Dessa forma, um critério difernte de seleção dos top-$k$ padrões pode ser definido de acordo com cada aplicação.
\par
Outras formas de ser compactar o conjunto de padrões freqüentes é desenvolver modelos alternativos de representação de tais conjuntos. Neste caso, o resultado obtido pela aplicação não é, necessariamente, constituído de um conjunto de padrões. Em \cite{boulicaut03freesets}, por exemplo, os autores introduzem uma estrutura chamada \textit{free-sets} de onde é possível aproximar o suporte de qualquer padrão da base. Os experimentos demonstram que a obtenção de \textit{free-sets} é possível mesmo quando a extração dos padrões freqüentes se torna intratável. Em \cite{DBLP:conf/icde/ZhuYHYC07} são introduzidos os chamados padrões colossais, e uma nova metodologia de mineração de padrões, que aproxima o conjunto resultado do conjunto de padrões freqüentes colossais. Note que, neste caso, o resultado do algoritmo é constituído, não do conjunto de padrões colossais, mas sim de uma aproximação deste conjunto.
\par
Nesta dissertação, estamos interessados em diminuir o tamanho do conjunto-solução como foi feito nos primeiros trabalhos apresentados, ou seja, obtendo, como resultado, um sub-conjunto (top-$k$) de padrões a partir do conjunto original de padrões freqüentes.

\section{Diminuição de Redundância no Conjunto de Padrões Freqüentes}

Aqui apresentamos alguns trabalhos relacionados com a obtenção de conjuntos padrões freqüentes com baixa redundância. Em \cite{DBLP:conf/kdd/XinCYH06}, trabalho que já foi citado na seção anterior, os autores apresentam a aplicação do modelo de obtenção de top-$k$ padrões nos problemas de extração de termos de documentos e \textit{prefetch} de blocos em seqüências de acesso ao disco, utilizando uma função objetivo que relaciona duas métricas propostas - significância e redundância. A idéia é obter, como resultado, um conjunto de padrões com alta significância e baixa redundância, que represente bem todo o conjunto de padrões freqüentes. O artigo apresenta as duas métricas da função objetivo e descreve um algoritmo guloso que resolve o problema com ordem de complexidade de tempo $O(\log k)$.
\par
Em \cite{xin05mining} encontramos uma abordagem diferente para o problema, onde os padrões freqüentes e fechados são agrupados de acordo com uma medida de similaridade baseada em cobertura de transações da base, e, de cada agrupamento, um padrão que possui boa representatividade em relação aos outros elementos do seu conjunto deve ser escolhido e acrescentado à solução. O resultado é um sub-conjunto de padrões freqüentes e fechados com baixa redundância e alta representatividade em relação ao conjunto original.
\par
Em \cite{zaki07origami} encontramos um novo paradigma para obtenção de uma representação resumida do conjunto de padrões freqüentes aplicado ao problema de mineração de grafos. O artigo apresenta a formulação da obtenção do conjunto $\alpha$-ortogonal e $\beta$-representativo como um problema de otimização NP-Hard. Um conjunto de padrões é considerado $\alpha$-ortogonal se a similaridade entre os padrões que o compõem é menor ou igual a um determinado valor $\alpha$. Dado um conjunto de padrões, um sub-conjunto deste é $\beta$-representativo em relação aos elementos que não fazem parte dele se, para cada um destes elementos, existe um elemento no sub-conjunto cuja similaridade entre os dois é maior ou igual a $\beta$. O objetivo é obter sub-conjuntos de padrões não redundantes que possuem um certo nível de representatividade em relação ao conjunto original. Esta abordagem será discutida com mais detalhes na seção \ref{sec:ortogonalidade_origami}.
\par
Nesta dissertação, estamos interessados em minimizar o conjunto de padrões freqüentes com o auxílio de uma função objetivo a ser definida de acordo com a aplicação. No nosso caso, a aplicação utilizada é a classificação associativa, e a função objetivo é a medida de ortogonalidade do conjunto, que está relacionada com a métrica de ortogonalidade utilizada.

\section{Classificação Associativa}

Dentre todos os modelos de classificação associativa encontrados na literatura, optamos por utilizar, neste trabalho, um modelo baseado na abordagem \textit{lazy}, introduzida em \cite{Veloso06Lazy}. Este artigo apresenta as vantagens do modelo \textit{lazy} quando comparado com a abordagem \textit{eager}, bastante explorada em árvores de decisão e classificadores baseados em entropia. Em primeiro lugar, artigo demonstra que classificadores associativos \textit{eager} possuem desempenho melhor que árvores de decisão, e discute como regras de decisão podem ser geradas a partir de um modelo baseado em árvores de decisão. Em seguida, os autores apresentam a abordagem \textit{lazy}, e demonstram que ela produz resultados melhores que a abordagem \textit{eager}. Classificadores \textit{eager} geram regras antes que as instâncias de teste sejam analisadas. A dificuldade de se antecipar todas as direções possíveis da tarefa de classificação faz com que a abordagem produza regras muito generalizadas, o que pode reduzir a eficácia do classificador. A estratégia \textit{lazy}, no entanto, só produz regras aplicáveis à instância de teste, o que faz com que a eficácia do classificador não seja penalizado pela generalização. Os resultados apresentados compravam a superioridade da estratégia \textit{lazy}.
\par
Em \cite{veloso06multi}, os autores utilizam a abordagem \textit{lazy} de classificação associativa direcionado para classificação de documentos. O artigo apresenta um algoritmo de classificação capaz de analisar tanto o conteúdo dos documentos quanto a existências de \textit{links} de saída e de entrada para outros documentos. As inovações introduzidas pelos autores produzem resultados mais efetivos e de maior eficácia que abordagens do estado da arte nas coleções utilizadas. A mesma abordagem ainda obteve resultados satisfatórios em outras aplicações \citep{veloso06, Veloso2007}.
\par
No nosso trabalho, aplicamos o conceito de ortogonalidade num modelo de classificação associativa baseada na abordagem \textit{lazy} e direcionado para a classificação de transações em bases de dados genéricas.

%Apesar da compactação do conjunto de padrões freqüentes resultar, em grande parte das ocasiões, na diminuição da redundância do conjunto, são poucos os trabalhos que possuem, como finalidade, soluções que maximizam as duas funções objetivo. Em \cite{pasquier99}, por exemplo, os autores apresentam a definição de padrões fechados, e demonstram que o problema de se encontrar o conjunto de padrões freqüentes numa base de dados pode ser reduzido ao problema de se encontrar o conjunto de padrões freqüentes fechados.
%
%Na literatura, encontramos diversos trabalhos relacionados, tanto com classificação associativa quanto com a obtenção de top-$k$ padrões como forma de compactar o conjunto de padrões freqüentes obtidos de uma base de dados e simplificar a visualização dos resultados, sendo que alguns destes trabalhos também estão relacionados com o termo ortogonalidade.
%\par
%Em \cite{DBLP:conf/kdd/XinCYH06}, os autores introduzem o problema da extração dos top-$k$ padrões livres de redundância através de um modelo que integra duas métricas - significância e relevância - numa única função objetivo. A idéia é obter, a partir do conjunto de todos os padrões freqüentes obtidos por uma abordagem qualquer de mineração de dados, um conjunto de $k$ padrões de alta significância e baixa redundância entre seus elementos. Este conjunto deve ser obtido com o auxílio de uma função objetivo que relaciona as duas métricas propostas. O resultado é um conjunto de padrões não similares que representam bem todo o conjunto de padrões freqüentes. O artigo apresenta duas funções objetivo, e descreve um algoritmo guloso que resolve o problema com ordem de complexidade de tempo $O(\log k)$.
%\par
%Em \cite{Veloso06Lazy} é introduzida a abordagem \textit{lazy} de classificação associativa, apresentando as vantagens deste modelo quando comparado com a abordagem \textit{eager}, bastante explorada em árvores de decisão e classificadores baseados em entropia. Em primeiro lugar, artigo demonstra que classificadores associativos \textit{eager} possuem desempenho melhor que árvores de decisão, e discute como regras de decisão podem ser geradas a partir de um modelo baseado em árvores de decisão. Em seguida, os autores apresentam a abordagem \textit{lazy}, e demonstram que ela produz resultados melhores que a abordagem \textit{eager}. Classificadores \textit{eager} geram regras antes que as instâncias de teste sejam analisadas. A dificuldade de se antecipar todas as direções possíveis da tarefa de classificação faz com que a abordagem produza regras muito generalizadas, o que pode reduzir a eficácia do classificador. A estratégia \textit{lazy}, no entanto, só produz regras aplicáveis à instância de teste, o que faz com que a eficácia do classificador não seja penalizado pela generalização. Os resultados apresentados compravam a superioridade da estratégia \textit{lazy}.
%\par
%Em \cite{veloso06multi}, os autores utilizam a abordagem \textit{lazy} de classificação associativa direcionado para classificação de documentos. O artigo apresenta um algoritmo de classificação capaz de analisar tanto o conteúdo dos documentos quanto a existências de \textit{links} de saída e de entrada para outros documentos. As inovações introduzidas pelos autores produzem resultados mais efetivos e de maior eficácia que abordagens do estado da arte nas coleções utilizadas.
%\par
%Em \cite{zaki07origami} encontramos um novo paradigma para obtenção de uma representação resumida do conjunto de padrões freqüentes. O artigo introduz uma metodologia randômica para obtenção de padrões maximais que possui, como principal característica, a possibilidade de cobrir uniformemente o espaço dos padrões, e ainda apresenta a formulação da obtenção do conjunto $\alpha$-ortogonal e $\beta$-representativo como um problema de otimização NP-Hard. Um conjunto de padrões é considerado $\alpha$-ortogonal se a similaridade entre os padrões que o compõem é menor ou igual a um determinado valor $\alpha$. Dado um conjunto de padrões, um sub-conjunto deste é $\beta$-representativo em relação aos elementos que não fazem parte dele se, para cada um destes elementos, existe um elemento no sub-conjunto cuja similaridade entre os dois é maior ou igual a $\beta$. O objetivo é obter sub-conjuntos de padrões não redundantes que possuem um certo nível de representatividade em relação ao conjunto original. Esta abordagem será discutida com mais detalhes na seção \ref{sec:ortogonalidade_origami}.
%\par
%O problema de compactação do conjunto de padrões freqüentes é explorado de diversas outras maneiras: \cite{TheobaldSW_BTW07} apresenta um \textit{framework} para indexação e recuperação em grandes coleções de dados estruturados, semi-estruturados e não estruturados. Em \cite{DBLP:conf/icde/ZhuYHYC07}, encontramos uma nova metodologia de mineração de padrões, que aproxima o conjunto resultado do conjunto de padrões freqüentes colossais. Em \cite{boulicaut03freesets} os autores introduzem uma estrutura chamada \textit{free-sets} de onde é possível aproximar o suporte de qualquer padrão da base. Os experimentos demonstram que a obtenção de \textit{free-sets} é possível mesmo quando a extração dos padrões freqüentes se torna intratável. Em \cite{fu00} encontramos uma nova forma de se obter o conjunto de padrões freqüentes sem a especificação de um suporte mínimo. Nesta abordagem, o usuário deve simplesmente informar a quantidade de itens que deseja no conjunto-solução. Já em \cite{DBLP:conf/icdm/HanWLT02}, a abordagem de obtenção do conjunto-solução sem especificação de suporte é aplicada à mineração de padrões fechados. Neste caso, o usuário deve informar, além do tamanho desejável do conjunto-solução, o tamanho mínimo dos padrões, e, finalmente, \cite{xin05mining} apresenta o problema de compressão de padrões com o objetivo de encontrar um conjunto de padrões de tamanho mínimo que possua pelo menos um representante para cada padrão freqüente do conjunto real.
%\par
%Também encontramos muitos outros trabalhos relacionados com o problema da classificação: Em \cite{liu98integrating} é apresentada a proposta de integração entre as áreas mineração de regras de associação e classificação baseada em regras. \cite{zaki99charm} demonstra que é possível gerar regras de associação utilizando apenas padrões fechados, sem perda de informação, e apresenta um eficiente algoritmo para mineração destes padrões. Em \cite{DBLP:conf/sdm/WangK05} encontramos um algoritmo de geração de regras com o objetivo de incluir, no conjunto-resultado, apenas as regras de alta confiança encontradas para cada instância da base de teste. Em \cite{li01cmar} os autores apresentam um novo método de classificação associativa baseada em regras de múltipla associação. O algoritmo realiza a classificação de uma instância de teste com o auxílio de um sub-conjunto de regras que casam com a instância. Em \cite{DBLP:conf/sdm/YinH03}, encontramos um algoritmo guloso que gera as regras de associação diretamente da base de treinamento. \cite{kuok98mining} introduz regras de associação baseadas em \textit{fuzzy sets}. Em \cite{DBLP:conf/icde/ChengYHH07} os autores realizam um estudo do problema de classificação e apresentam um \textit{framework} que relaciona padrões freqüentes com métricas discriminativas tais como ganho de informação, e, finalmente, \cite{DBLP:conf/icde/LentSW97} apresenta um algoritmo de classificação \textit{multi-label} com a intenção de reduzir a redundância de informações durante o processo de aprendizado existente nas propostas anteriores.
%\par
%E ainda, a abordagem \textit{lazy} proposta por \cite{Veloso06Lazy}, e utilizada em \cite{veloso06multi}, também obteve resultados satisfatórios em outras aplicações \citep{veloso06, Veloso2007}.